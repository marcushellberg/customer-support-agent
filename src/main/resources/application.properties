quarkus.http.port=${PORT:8080}
quarkus.live-reload.instrumentation=true
vaadin.hilla.live-reload.enable=true

# LangChain4j properties
quarkus.langchain4j.chat-model.provider=openai
quarkus.langchain4j.embedding-model.provider=openai
quarkus.langchain4j.chat-memory.type=token-window
quarkus.langchain4j.chat-memory.token-window.max-tokens=1000
quarkus.langchain4j.openai.api-key=${OPENAI_API_KEY:dummy}
quarkus.langchain4j.openai.chat-model.model-name=gpt-4o-mini
quarkus.langchain4j.openai.chat-model.temperature=0

# Set -Dquarkus.profile=ollama to enable ollama instead of Open AI
quarkus.langchain4j.ollama.chat-model.temperature=0
quarkus.langchain4j.ollama.chat-model.model-id=llama3.2
quarkus.langchain4j.ollama.timeout=10m
%ollama.quarkus.langchain4j.chat-model.provider=ollama
%ollama.quarkus.langchain4j.chat-memory.type=message-window
%ollama.quarkus.langchain4j.chat-memory.memory-window.max-messages=10
%ollama.quarkus.langchain4j.embedding-model.provider=ollama

# RAG properties
quarkus.langchain4j.easy-rag.path=src/main/resources
quarkus.langchain4j.easy-rag.recursive=false
quarkus.langchain4j.easy-rag.path-matcher=glob:terms-of-service.txt
quarkus.langchain4j.easy-rag.reuse-embeddings.enabled=true
quarkus.langchain4j.easy-rag.max-results=2
quarkus.langchain4j.easy-rag.min-score=0.6
quarkus.langchain4j.easy-rag.max-segment-size=50
quarkus.langchain4j.easy-rag.max-overlap-size=0

# Logging properties
quarkus.langchain4j.log-requests=true
quarkus.langchain4j.log-responses=true
quarkus.log.category."org.vaadin.marcus".level=DEBUG
quarkus.log.category."dev.langchain4j".level=DEBUG
quarkus.log.category."dev.ai4j.openai4j".level=DEBUG
quarkus.log.category."org.atmosphere".level=WARN
quarkus.log.category."io.quarkus.opentelemetry.runtime.exporter.otlp.sender".level=OFF
quarkus.log.category."io.opentelemetry.exporter.internal.grpc".level=OFF
%dev,test.quarkus.log.console.level=DEBUG